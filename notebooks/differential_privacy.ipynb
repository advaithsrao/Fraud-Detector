{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !poetry run pip3 install --force-reinstall opacus==0.13.0\n",
    "# !poetry update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/woodyx218/opacus_global_clipping.git\n",
    "# !mv opacus_global_clipping ./ethics/\n",
    "# !pip3 install -e ./ethics/opacus_global_clipping\n",
    "# !rm -rf ./ethics/opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ./ethics/opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !poetry run pip3 install --upgrade opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /common/home/ps1279/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ethics.differential_privacy import DistilbertPrivacyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /common/home/ps1279/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertModel\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "from mlflow.sklearn import save_model\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from utils.util_modeler import Word2VecEmbedder, TPSampler\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\n",
    "#         '../ethics'\n",
    "# )\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "# from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = nn.Linear(self.distilbert.config.hidden_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Apply pre-classification layer\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = self.relu(pooler)\n",
    "\n",
    "        # Apply final classification layer\n",
    "        output = self.classifier(pooler)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilbertPrivacyModel:\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_labels=2, \n",
    "        path='', \n",
    "        model_name='distilbert-base-uncased', \n",
    "        learning_rate=2e-5, \n",
    "        epsilon=1e-8, \n",
    "        num_epochs=40, \n",
    "        batch_size=128, \n",
    "        device=None\n",
    "    ):\n",
    "        self.num_labels = num_labels\n",
    "        self.path = path\n",
    "        self.model_name = model_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        if not self.device and torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "        elif not self.device:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        self.device = torch.device(self.device)\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        if self.path != '':\n",
    "            raise NotImplementedError('Loading model from path is not implemented yet.')\n",
    "        else:\n",
    "            self.model = BaseModel()\n",
    "            self.model.to(self.device)\n",
    "        \n",
    "        self.privacy_engine = PrivacyEngine()\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        body: pd.Series | list[str], \n",
    "        label: pd.Series | list[int], \n",
    "        validation_size=0.2,\n",
    "        wandb=None\n",
    "    ):\n",
    "        \"\"\"Trains the model using the given data.\n",
    "\n",
    "        Args:\n",
    "            body (pd.Series | list[str]): The body of the email.\n",
    "            label (pd.Series | list[int]): The label of the email.\n",
    "            validation_size (float, optional): The size of the validation set. Defaults to 0.2.\n",
    "            wandb (wandb, optional): The wandb object. Defaults to None. If given, logs the training process to wandb.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the body and label are not of the same size.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(body, pd.Series):\n",
    "            body = body.tolist()\n",
    "        if isinstance(label, pd.Series):\n",
    "            label = label.tolist()\n",
    "\n",
    "        # Tokenize input texts and convert labels to tensors\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        label_ids = []\n",
    "\n",
    "        for _body, _label in zip(body, label):\n",
    "            # Tokenize the input text using the Roberta tokenizer\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                _body,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            input_ids.append(inputs['input_ids'])\n",
    "            attention_masks.append(inputs['attention_mask'])\n",
    "            label_ids.append(torch.tensor(_label))  # Convert the label to a tensor\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        label_ids = torch.stack(label_ids)\n",
    "\n",
    "        # Split the data into train and validation sets\n",
    "        dataset = TensorDataset(input_ids, attention_masks, label_ids)\n",
    "        dataset_size = len(dataset)\n",
    "        val_size = int(validation_size * dataset_size)\n",
    "        train_size = dataset_size - val_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        # # Create data loaders for training and validation data\n",
    "        # SAMPLE_RATE = self.batch_size / len(train_dataset)\n",
    "\n",
    "        # train_sampler=UniformWithReplacementSampler(\n",
    "        #     num_samples=len(train_dataset),\n",
    "        #     sample_rate=SAMPLE_RATE,\n",
    "        # )\n",
    "        \n",
    "        # train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "        # Initialize the Privacy engine, optimizer and learning rate scheduler\n",
    "        optimizer = AdamW(list(self.model.parameters()), lr=self.learning_rate, eps=self.epsilon)\n",
    "        \n",
    "        total_steps = len(train_dataloader) * self.num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        MAX_GRAD_NORM = 0.1\n",
    "\n",
    "        # m = self.save_model\n",
    "        # o = optimizer\n",
    "        # t = train_dataloader\n",
    "\n",
    "\n",
    "        self.model, optimizer, train_dataloader = self.privacy_engine.make_private_with_epsilon(\n",
    "            module=self.model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_dataloader,\n",
    "            target_delta=1/len(train_dataloader),\n",
    "            target_epsilon=self.epsilon, \n",
    "            epochs=self.num_epochs,\n",
    "            max_grad_norm=MAX_GRAD_NORM,\n",
    "        )\n",
    "\n",
    "        # return m, o, t, self.model, optimizer, train_dataloader\n",
    "\n",
    "        \n",
    "\n",
    "        # self.privacy_engine = PrivacyEngine(\n",
    "        #     module=self.model,\n",
    "        #     sample_rate=SAMPLE_RATE,\n",
    "        #     target_delta = 1 / len(train_dataloader),\n",
    "        #     target_epsilon = self.epsilon, \n",
    "        #     epochs = self.num_epochs,\n",
    "        #     max_grad_norm=MAX_GRAD_NORM,\n",
    "        # )\n",
    "\n",
    "        # self.privacy_engine.attach(optimizer)\n",
    "\n",
    "        # Initialize variables for early stopping\n",
    "        best_validation_loss = float(\"inf\")\n",
    "        patience = 5  # Number of epochs to wait for improvement\n",
    "        wait = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f'{\"=\"*20} Epoch {epoch + 1}/{self.num_epochs} {\"=\"*20}')\n",
    "\n",
    "            # Training loop\n",
    "            self.model.train()\n",
    "            \n",
    "            total_train_loss = 0\n",
    "\n",
    "            with BatchMemoryManager(\n",
    "                data_loader=train_dataloader, \n",
    "                max_physical_batch_size=self.batch_size, \n",
    "                optimizer=optimizer\n",
    "            ) as memory_safe_data_loader:\n",
    "                for step, batch in enumerate(memory_safe_data_loader):\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    b_input_ids = batch[0].to(self.device)\n",
    "                    b_input_mask = batch[1].to(self.device)\n",
    "                    b_labels = batch[2].to(self.device, dtype=torch.float)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = self.model(b_input_ids, attention_mask=b_input_mask)\n",
    "                    \n",
    "                    predictions = outputs.squeeze()\n",
    "\n",
    "                    # Calculate binary cross-entropy loss\n",
    "                    loss = F.binary_cross_entropy_with_logits(predictions, b_labels)\n",
    "\n",
    "                    total_train_loss += loss.item()\n",
    "\n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(list(self.model.parameters()), 1.0)\n",
    "\n",
    "                    print('5:', optimizer)\n",
    "                    optimizer.step()\n",
    "                    print('6:', optimizer)\n",
    "\n",
    "                    # Update the learning rate\n",
    "                    print('7:', scheduler)\n",
    "                    scheduler.step()\n",
    "                    print('8:', scheduler)\n",
    "\n",
    "                    if step % 100 == 0 and step != 0:\n",
    "                        avg_train_loss = total_train_loss / 100\n",
    "                        print(f'Step {step}/{len(train_dataloader)} - Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "                        total_train_loss = 0\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            print(f'Training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "            epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(delta=1e-5)\n",
    "            print(f\"For Model, ε = {epsilon:.2f}, δ = 1e-5 for α = {best_alpha}\")\n",
    "\n",
    "            # Evaluation loop\n",
    "            self.model.eval()\n",
    "            total_eval_accuracy = 0\n",
    "            total_eval_loss = 0\n",
    "\n",
    "            for batch in validation_dataloader:\n",
    "                b_input_ids = batch[0].to(self.device)\n",
    "                b_input_mask = batch[1].to(self.device)\n",
    "                b_labels = batch[2].to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "                    predictions = outputs.squeeze()\n",
    "\n",
    "                    # Calculate binary cross-entropy loss\n",
    "                    loss = F.binary_cross_entropy_with_logits(predictions, b_labels)\n",
    "\n",
    "                    total_eval_loss += loss.item()\n",
    "\n",
    "                total_eval_accuracy += self.accuracy(predictions, b_labels)\n",
    "\n",
    "            avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "            print(f'Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "\n",
    "            avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "            if wandb is not None:\n",
    "                wandb.log({\n",
    "                    'epoch': epoch, \n",
    "                    'train_loss': avg_train_loss, \n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'val_accuracy': avg_val_accuracy,\n",
    "                })\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_val_loss < best_validation_loss:\n",
    "                best_validation_loss = avg_val_loss\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "\n",
    "            if wait >= patience:\n",
    "                print(f'Early stopping after {patience} epochs without improvement.')\n",
    "                break\n",
    "\n",
    "    def predict(\n",
    "        self, \n",
    "        body: pd.Series | list[str]\n",
    "    ):\n",
    "        \"\"\"Predicts the labels of the given data.\n",
    "\n",
    "        Args:\n",
    "            body (pd.Series | list[str]): The body of the email.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The predictions of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # If input_texts is a Pandas Series, convert it to a list\n",
    "        if isinstance(body, pd.Series):\n",
    "            body = body.tolist()\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for _body in body:\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                _body,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            input_ids.append(inputs['input_ids'])\n",
    "            attention_masks.append(inputs['attention_mask'])\n",
    "\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_input_mask = batch[1].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "            class_predictions = (outputs >= 0.5).detach().cpu().numpy()\n",
    "            predictions.extend(class_predictions.tolist())\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def save_model(\n",
    "            self,\n",
    "            path: str\n",
    "    ):\n",
    "        \"\"\"Saves the model to the given path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to save the model to.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the directory exists, and if not, create it\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "        # Save the transformer model and the classification head\n",
    "        self.model.save_pretrained(path)\n",
    "        torch.save(self.classification_head.state_dict(), os.path.join(path, 'classification_head.pth'))\n",
    "    \n",
    "    def accuracy(\n",
    "        self, \n",
    "        preds, \n",
    "        labels\n",
    "    ):\n",
    "        \"\"\"Calculates the accuracy of the model.\n",
    "\n",
    "        Args:\n",
    "            preds (torch.Tensor|numpy.ndarray): The predictions of the model.\n",
    "            labels (torch.Tensor|numpy.ndarray): The labels of the data.\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(preds, np.ndarray):\n",
    "            preds = torch.from_numpy(preds)\n",
    "        if isinstance(labels, np.ndarray):\n",
    "            labels = torch.from_numpy(labels)\n",
    "        \n",
    "        preds = (preds >= 0.5).float()\n",
    "        accuracy = (y_pred_test == y_test_tensor).float().mean()\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('./data/fraud_detector_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data.Source == 'Phishing Data']\n",
    "\n",
    "# data = pd.concat(\n",
    "#     [data[data.Label == 1].head(100),\n",
    "#     data[data.Label == 0].head(100)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/ps1279/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = DistilbertPrivacyModel(\n",
    "    num_epochs=1,\n",
    "    epsilon=1e-8,\n",
    "    batch_size=8,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/ps1279/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/common/home/ps1279/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/common/home/ps1279/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 1/1 ====================\n",
      "5: AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-08\n",
      "    initial_lr: 2e-05\n",
      "    lr: 2e-05\n",
      "    weight_decay: 0.0\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/ps1279/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4] at entry 0 and [1] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     body\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mBody,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     label\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mLabel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters()), \u001b[39m1.0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m5:\u001b[39m\u001b[39m'\u001b[39m, optimizer)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m6:\u001b[39m\u001b[39m'\u001b[39m, optimizer)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X10sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39m# Update the learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/optimizers/optimizer.py:513\u001b[0m, in \u001b[0;36mDPOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m    511\u001b[0m         closure()\n\u001b[0;32m--> 513\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_step():\n\u001b[1;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    515\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/optimizers/optimizer.py:494\u001b[0m, in \u001b[0;36mDPOptimizer.pre_step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpre_step\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m, closure: Optional[Callable[[], \u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    486\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m    Perform actions specific to ``DPOptimizer`` before calling\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m    underlying  ``optimizer.step()``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39m            returns the loss. Optional for most optimizers.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip_and_accumulate()\n\u001b[1;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_skip_next_step():\n\u001b[1;32m    496\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_last_step_skipped \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/optimizers/optimizer.py:404\u001b[0m, in \u001b[0;36mDPOptimizer.clip_and_accumulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     per_param_norms \u001b[39m=\u001b[39m [\n\u001b[1;32m    402\u001b[0m         g\u001b[39m.\u001b[39mreshape(\u001b[39mlen\u001b[39m(g), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnorm(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_samples\n\u001b[1;32m    403\u001b[0m     ]\n\u001b[0;32m--> 404\u001b[0m     per_sample_norms \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(per_param_norms, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mnorm(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    405\u001b[0m     per_sample_clip_factor \u001b[39m=\u001b[39m (\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm \u001b[39m/\u001b[39m (per_sample_norms \u001b[39m+\u001b[39m \u001b[39m1e-6\u001b[39m)\n\u001b[1;32m    407\u001b[0m     )\u001b[39m.\u001b[39mclamp(\u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n\u001b[1;32m    409\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4] at entry 0 and [1] at entry 1"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    body=data.Body,\n",
    "    label=data.Label,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Per sample gradient is not initialized. Not updated in backward pass?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arao/Local/Github/Fraud-Detector/notebooks/differential_privacy.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m e\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/optimizers/optimizer.py:513\u001b[0m, in \u001b[0;36mDPOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m    511\u001b[0m         closure()\n\u001b[0;32m--> 513\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_step():\n\u001b[1;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    515\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/optimizers/optimizer.py:494\u001b[0m, in \u001b[0;36mDPOptimizer.pre_step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpre_step\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m, closure: Optional[Callable[[], \u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    486\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39m    Perform actions specific to ``DPOptimizer`` before calling\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m    underlying  ``optimizer.step()``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39m            returns the loss. Optional for most optimizers.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip_and_accumulate()\n\u001b[1;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_skip_next_step():\n\u001b[1;32m    496\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_last_step_skipped \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/optimizers/optimizer.py:397\u001b[0m, in \u001b[0;36mDPOptimizer.clip_and_accumulate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclip_and_accumulate\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    392\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39m    Performs gradient clipping.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39m    Stores clipped and aggregated gradients into `p.summed_grad```\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad_samples[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    398\u001b[0m         \u001b[39m# Empty batch\u001b[39;00m\n\u001b[1;32m    399\u001b[0m         per_sample_clip_factor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m0\u001b[39m,))\n\u001b[1;32m    400\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/optimizers/optimizer.py:345\u001b[0m, in \u001b[0;36mDPOptimizer.grad_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m ret \u001b[39m=\u001b[39m []\n\u001b[1;32m    344\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams:\n\u001b[0;32m--> 345\u001b[0m     ret\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_flat_grad_sample(p))\n\u001b[1;32m    346\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/ethical-fraud-detector-qnRNkJHZ-py3.10/lib/python3.10/site-packages/opacus/optimizers/optimizer.py:282\u001b[0m, in \u001b[0;36mDPOptimizer._get_flat_grad_sample\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPer sample gradient not found. Are you using GradSampleModule?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad_sample \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPer sample gradient is not initialized. Not updated in backward pass?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(p\u001b[39m.\u001b[39mgrad_sample, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    286\u001b[0m     ret \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mgrad_sample\n",
      "\u001b[0;31mValueError\u001b[0m: Per sample gradient is not initialized. Not updated in backward pass?"
     ]
    }
   ],
   "source": [
    "e.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<opacus.data_loader.DPDataLoader at 0x7efd79f84130>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel()\n",
    "# list(model.parameters())\n",
    "# torch.nn.utils.clip_grad_norm_(list(model.parameters()), 1.0)\n",
    "# list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
