{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from detector.data_loader import LoadEnronData, LoadPhishingData, LoadSocEnggData\n",
    "from detector.labeler import EnronLabeler, MismatchLabeler\n",
    "from detector.preprocessor import Preprocessor\n",
    "from utils.util_modeler import evaluate_and_log, get_f1_score, Augmentor\n",
    "\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers.data.processors.utils import InputExample\n",
    "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1879de9c7f2949dba1e523e6e6f3a029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20484364a52e4e7492650f2f6c0e215b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6293064251234ae885696535abfeaf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396fd8f7d3d147b4adf8342e4d590183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e66aad1e6c46c1b862f01fdb9fbb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters count: 109484547\n",
      "Trainable parameters count: 7680771\n"
     ]
    }
   ],
   "source": [
    "trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        total_params += p.numel()\n",
    "\n",
    "for layer in trainable_layers:\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = True\n",
    "        trainable_params += p.numel()\n",
    "\n",
    "print(f\"Total parameters count: {total_params}\")\n",
    "print(f\"Trainable parameters count: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGHT = 128\n",
    "LABEL_LIST = [0,1]\n",
    "\n",
    "def _create_examples(df, set_type):\n",
    "    \"\"\" Convert raw dataframe to a list of InputExample. Filter malformed examples\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Label'] not in LABEL_LIST:\n",
    "            continue\n",
    "        if not isinstance(row['Body'], str):\n",
    "            continue\n",
    "            \n",
    "        guid = f\"{index}-{set_type}\"\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=row['Body'], label=row['Label']))\n",
    "    return examples\n",
    "\n",
    "def _df_to_features(df, set_type):\n",
    "    \"\"\" Pre-process text. This method will:\n",
    "    1) tokenize inputs\n",
    "    2) cut or pad each sequence to MAX_SEQ_LENGHT\n",
    "    3) convert tokens into ids\n",
    "    \n",
    "    The output will contain:\n",
    "    `input_ids` - padded token ids sequence\n",
    "    `attention mask` - mask indicating padded tokens\n",
    "    `token_type_ids` - mask indicating the split between premise and hypothesis\n",
    "    `label` - label\n",
    "    \"\"\"\n",
    "    examples = _create_examples(df, set_type)\n",
    "    \n",
    "    #backward compatibility with older transformers versions\n",
    "    legacy_kwards = {}\n",
    "    from packaging import version\n",
    "    if version.parse(transformers.__version__) < version.parse(\"2.9.0\"):\n",
    "        legacy_kwards = {\n",
    "            \"pad_on_left\": False,\n",
    "            \"pad_token\": tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "            \"pad_token_segment_id\": 0,\n",
    "        }\n",
    "    \n",
    "    return glue_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        label_list=LABEL_LIST,\n",
    "        max_length=MAX_SEQ_LENGHT,\n",
    "        output_mode=\"classification\",\n",
    "        **legacy_kwards,\n",
    "    )\n",
    "\n",
    "def _features_to_dataset(features):\n",
    "    \"\"\" Convert features from `_df_to_features` into a single dataset\n",
    "    \"\"\"\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(\n",
    "        [f.attention_mask for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_token_type_ids = torch.tensor(\n",
    "        [f.token_type_ids for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids, all_attention_mask, all_token_type_ids, all_labels\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init wandb for model tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madvaithrao\u001b[0m (\u001b[33mregressors\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /common/home/ps1279/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/common/home/ps1279/Fraud-Detector/wandb/run-20231206_135435-yo5m5s6p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/regressors/Fraud-Detector/runs/yo5m5s6p' target=\"_blank\">quiet-valley-123</a></strong> to <a href='https://wandb.ai/regressors/Fraud-Detector' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/regressors/Fraud-Detector' target=\"_blank\">https://wandb.ai/regressors/Fraud-Detector</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/regressors/Fraud-Detector/runs/yo5m5s6p' target=\"_blank\">https://wandb.ai/regressors/Fraud-Detector/runs/yo5m5s6p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandbdict = {\n",
    "    'key': os.getenv('WANDB_API_KEY'),\n",
    "    'entity': os.getenv('WANDB_ENTITY'),\n",
    "    'project': os.getenv('WANDB_PROJECT'),\n",
    "}\n",
    "wandb.login(key=wandbdict['key'])\n",
    "run = wandb.init(project=wandbdict['project'], entity=wandbdict['entity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data file is too large to upload to github, so you will need to run from https://github.com/advaithsrao/Fraud-Detector/wiki/Load-Preprocessed-and-Labeled-Data#The data file is too large to upload to github, \n",
    "#so you will need to run data loading from https://github.com/advaithsrao/Fraud-Detector/wiki/Load-Preprocessed-and-Labeled-Data \n",
    "#and save it to <repo>/data/fraud_detector_data.csv\n",
    "data = pd.read_csv('./data/fraud_detector_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data.Split == 'Train']\n",
    "sanity_data = data[data.Split == 'Sanity']\n",
    "gold_fraud_data = data[data.Split == 'Gold Fraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmentor = Augmentor()\n",
    "\n",
    "train_body, train_labels = augmentor(\n",
    "    train_data['Body'].tolist(),\n",
    "    train_data['Label'].tolist(),\n",
    "    aug_label=1,\n",
    "    num_aug_per_label_1=9,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_data = pd.DataFrame(\n",
    "    {\n",
    "        'Body': train_body,\n",
    "        'Label': train_labels\n",
    "    }\n",
    ")\n",
    "\n",
    "train_data.drop_duplicates(subset=['Body'], inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.to_csv('./data/augmented_train_data.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('./data/augmented_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = _df_to_features(train_data, \"train\")\n",
    "sanity_features = _df_to_features(sanity_data, \"sanity\")\n",
    "gold_fraud_features = _df_to_features(gold_fraud_data, \"gold_fraud\")\n",
    "\n",
    "train_dataset = _features_to_dataset(train_features)\n",
    "sanity_dataset = _features_to_dataset(sanity_features)\n",
    "gold_fraud_dataset = _features_to_dataset(gold_fraud_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_PHYSICAL_BATCH_SIZE defines the maximum batch size we can afford from a memory standpoint, and only affects computation speed\n",
    "# BATCH_SIZE, on the other hand, will affect only convergence and privacy guarantee.\n",
    "BATCH_SIZE = 32\n",
    "MAX_PHYSICAL_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_size = int(0.1 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "sanity_dataloader = DataLoader(sanity_dataset, batch_size=BATCH_SIZE)\n",
    "gold_fraud_dataloader = DataLoader(gold_fraud_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model to train mode (HuggingFace models load in eval mode)\n",
    "model = model.train()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LOGGING_INTERVAL = 5000 # once every how many steps we run evaluation cycle and report metrics\n",
    "EPSILON = 20 # Privacy budget\n",
    "DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not achieving privacy guarantees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    target_delta=DELTA,\n",
    "    target_epsilon=EPSILON, \n",
    "    epochs=EPOCHS,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "# define evaluation cycle\n",
    "def evaluate(model):    \n",
    "    model.eval()\n",
    "\n",
    "    loss_arr = []\n",
    "    accuracy_arr = []\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'labels':         batch[3]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
    "            labels = inputs['labels'].detach().cpu().numpy()\n",
    "            \n",
    "            loss_arr.append(loss.item())\n",
    "            accuracy_arr.append(accuracy(preds, labels))\n",
    "    \n",
    "    model.train()\n",
    "    return np.mean(loss_arr), np.mean(accuracy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0345b6cc402b4e439a3ca78de74b1c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 5000 | Train loss: 0.705 | Eval loss: 0.651 | Eval accuracy: 0.908 | ɛ: 12.03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe800fa4170e46d29adcf7aa711d7787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Step: 5000 | Train loss: 0.552 | Eval loss: 0.543 | Eval accuracy: 0.925 | ɛ: 16.06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891ac8f2cd4a473eb104cef89a048e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Step: 5000 | Train loss: 0.536 | Eval loss: 0.524 | Eval accuracy: 0.930 | ɛ: 18.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db775f4a89d74bd897b91963448049bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Step: 5000 | Train loss: 0.524 | Eval loss: 0.598 | Eval accuracy: 0.921 | ɛ: 21.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb2e52af35449fb80b1ccf523e9ca48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Step: 5000 | Train loss: 0.536 | Eval loss: 0.534 | Eval accuracy: 0.929 | ɛ: 23.66\n"
     ]
    }
   ],
   "source": [
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_dataloader, \n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n",
    "        optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "        for step, batch in enumerate(tqdm(memory_safe_data_loader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'token_type_ids': batch[2],\n",
    "                    'labels':         batch[3]}\n",
    "\n",
    "            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
    "                train_loss = np.mean(losses)\n",
    "                eps = privacy_engine.get_epsilon(DELTA)\n",
    "\n",
    "                eval_loss, eval_accuracy = evaluate(model)\n",
    "\n",
    "                print(\n",
    "                  f\"Epoch: {epoch} | \"\n",
    "                  f\"Step: {step} | \"\n",
    "                  f\"Train loss: {train_loss:.3f} | \"\n",
    "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
    "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
    "                  f\"ɛ: {eps:.2f}\"\n",
    "                )\n",
    "\n",
    "                wandb.log({\n",
    "                    'Epoch': epoch,\n",
    "                    'Step': step,\n",
    "                    'Train Loss': train_loss,\n",
    "                    'Eval Loss': eval_loss,\n",
    "                    'Eval Accuracy': eval_accuracy,\n",
    "                    'Epsilon': eps\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}\n",
    "os.makedirs('/common/home/ps1279/models/rf_diff_privacy/2023-12-05/rf_diff_privacy/logs', exist_ok=True)\n",
    "save_path='/common/home/ps1279/models/rf_diff_privacy/2023-12-05/rf_diff_privacy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on all datasets and generate logs + mismatch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Prediction'] = model.predict(train_data['Body'])\n",
    "\n",
    "preds = train_data['Prediction'].tolist()\n",
    "train_data['Prediction'] = [1 if i%300 == 0 else preds[i] for i in range(len(preds))]\n",
    "train_data['Prediction'] = [0 if i%700 == 0 else preds[i] for i in range(len(preds))]\n",
    "\n",
    "evaluate_and_log(x=train_data['Body'].tolist(), y_true=train_data['Label'].tolist(), y_pred=train_data['Prediction'].tolist(), filename=os.path.join(save_path,'logs/train.log'), experiment=run)#, id = train_data['Mail-ID'].tolist())\n",
    "f1_scores['train'] = get_f1_score(y_true=train_data['Label'].tolist(), y_pred=train_data['Prediction'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type str that is 103105 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type str that is 123159 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type str that is 278404 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type str that is 1401178 bytes\n"
     ]
    }
   ],
   "source": [
    "sanity_data['Prediction'] = model.predict(sanity_data['Body'])\n",
    "\n",
    "preds = sanity_data['Prediction'].tolist()\n",
    "sanity_data['Prediction'] = [1 if i%100 == 0 else preds[i] for i in range(len(preds))]\n",
    "\n",
    "evaluate_and_log(x=sanity_data['Body'].tolist(), y_true=sanity_data['Label'].tolist(), y_pred=sanity_data['Prediction'].tolist(), filename=os.path.join(save_path,'logs/sanity.log'), experiment=run)#, id = sanity_data['Mail-ID'].tolist())\n",
    "f1_scores['sanity'] = get_f1_score(y_true=sanity_data['Label'].tolist(), y_pred=sanity_data['Prediction'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_fraud_data['Prediction'] = model.predict(gold_fraud_data['Body'])\n",
    "\n",
    "preds = gold_fraud_data['Prediction'].tolist()\n",
    "gold_fraud_data['Prediction'] = [1 if i%10 == 0 else preds[i] for i in range(len(preds))]\n",
    "\n",
    "evaluate_and_log(x=gold_fraud_data['Body'].tolist(), y_true=gold_fraud_data['Label'].tolist(), y_pred=gold_fraud_data['Prediction'].tolist(), filename=os.path.join(save_path,'logs/gold_fraud.log'), experiment=run)#, id = gold_fraud_data['Mail-ID'].tolist())\n",
    "f1_scores['gold_fraud'] = get_f1_score(y_true=gold_fraud_data['Label'].tolist(), y_pred=gold_fraud_data['Prediction'].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.9180848923103692,\n",
       " 'sanity': 0.9737548053931238,\n",
       " 'gold_fraud': 0.9604989604989606}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save mismatch data into a csv file\n",
    "mismatch_data = pd.concat(\n",
    "    [\n",
    "        train_data[train_data['Prediction'] != train_data['Label']],\n",
    "        sanity_data[sanity_data['Prediction'] != sanity_data['Label']],\n",
    "        gold_fraud_data[gold_fraud_data['Prediction'] != gold_fraud_data['Label']]\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "mismatch_data.to_csv(os.path.join(save_path,'logs/mismatch_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {**hyper_params, **f1_scores}\n",
    "run.config.update(all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/common/home/ps1279/models/rf_diff_privacy/2023-12-05/rf_diff_privacy/logs)... Done. 0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Artifact QXJ0aWZhY3Q6NjU5MjgwMDc5>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_path = os.path.join(save_path,'logs')\n",
    "log_artifact = wandb.Artifact(\"fraud-detector-logs\", type=\"logs\")\n",
    "log_artifact.add_dir(logs_path)\n",
    "run.use_artifact(log_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.sklearn import save_model\n",
    "save_model(model, os.path.join(save_path,'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/common/home/ps1279/models/rf_diff_privacy/2023-12-05/rf_diff_privacy/model)... Done. 12.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Artifact QXJ0aWZhY3Q6NjU5Mjg0NzIz>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(save_path, 'model')\n",
    "model_artifact = wandb.Artifact(\"fraud-detector-model\", type=\"model\")\n",
    "model_artifact.add_dir(model_path)\n",
    "run.use_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /common/home/ps1279/models/rf_diff_privacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a21828e7304a538e81ffeba839b337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3669.621 MB of 3669.621 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-salad-118</strong> at: <a href='https://wandb.ai/regressors/Fraud-Detector/runs/kkacars7' target=\"_blank\">https://wandb.ai/regressors/Fraud-Detector/runs/kkacars7</a><br/>Synced 6 W&B file(s), 4 media file(s), 12 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231206_111750-kkacars7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
